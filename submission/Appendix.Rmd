---
title: "Appendix"
author: "Jake Singleton"
date: "3/14/2022"
output: pdf_document
---

```{r}
set.seed(9)
```


# Data

Below we download all MLB 2021 regular season play-by-play data. The metadata is [here](https://www.retrosheet.org/eventfile.htm), and the sources for obtaining the data are [here](https://bayesball.github.io/VB/Simple_Retrosheet.html) and [here](https://www.pitchbypitch.com/2013/11/29/installing-chadwick-software-on-mac/).

```{r}
# Data cleaning
library(tidyverse)
library(data.table)

# Data
library(readr)
dat = read_csv("./data/all2021.csv", col_names = FALSE)  # Play-by-play data
fields = read.csv("./data/fields.csv")  # Header
names(dat) = fields[, "Header"]

head(dat, 10)
dim(dat)
```

We begin with a bit of data cleaning: specifically we create the column `ID`, a unique identifier of each half inning of each game. Then we create a new data frame `half_innings` in which we summarize the number of outs recorded in the half inning.

```{r}
dat = dat %>% 
  mutate(ID = paste(GAME_ID, INN_CT, BAT_HOME_ID))

half_innings = dat %>%
  group_by(ID) %>% 
  summarize(nOuts = sum(EVENT_OUTS_CT))
head(half_innings)
```

Now we filter the original data to only focus on batting plays and half innings in which 3 outs were recorded. We do the first filtering because, in order to calculate the number of runs scored moving from an old state to a new state in our Markov chain, we will need to assume the ball was put into play. Without doing so, stolen bases would be included--for example, the transition `0100` to `0010` could represent a stolen base. Then, by the definition of $\text{RUNS}=(\text{NRunners}_0 + O_0 + 1) - (\text{NRunners}_1 + O_1)$ , this we would count this transition as a run scored, which would not be true! However, since we've filtered for batted ball events only, this transition indeed can only represent one resulting in a run scored. We do the second filtering because we will simulate half innings from our Markov chain, where a half inning lasts until exactly 3 outs are made.

Furthermore, I will remove observations in extra innings, i.e. plays that took place in inning 10 or later OR plays that took place in the 8th inning or later of a doubleheader. This is because special baserunner rules are implemented in these later innings in order to help the game finish, and the data doesn't account for these scenarios. We call our new data frame `dat1`.

```{r}
# Filter out non-batted ball events, non-3 out innings, and extra innings
# First make extra innings flag
dat = dat %>%
  mutate(extra_inning_fl = ifelse((INN_CT > 9) | ((INN_CT > 7) & (as.numeric(substr(GAME_ID, 12, 12))) > 0), TRUE, FALSE))

dat1 = dat %>%
  inner_join(half_innings, by = "ID") %>%
  filter(BAT_EVENT_FL == TRUE, nOuts == 3, extra_inning_fl == FALSE)

dim(dat1)
```

# Markov Chain Computation

Now we may compute the desired Markov chain with the following function

```{r}
# Takes a data frame of CLEAN data and returns transition probability matrix
make_chain = function(df) {
  df = df %>%
  mutate(STATE0 = paste0(OUTS_CT, 
                         ifelse(is.na(BASE1_RUN_ID), 0, 1),
                         ifelse(is.na(BASE2_RUN_ID), 0, 1),
                         ifelse(is.na(BASE3_RUN_ID), 0, 1)),
         runner1st = as.numeric((RUN1_DEST_ID == 1) | (BAT_DEST_ID == 1)),
         runner2nd = as.numeric((RUN1_DEST_ID == 2) | (RUN2_DEST_ID == 2) | (BAT_DEST_ID == 2)),
         runner3rd = as.numeric((RUN3_DEST_ID == 3) | (RUN2_DEST_ID == 3) | (RUN1_DEST_ID == 3) | (BAT_DEST_ID == 3)),
         runners = paste0(runner1st, runner2nd, runner3rd),
         STATE1 = paste0(OUTS_CT + EVENT_OUTS_CT,  # outs in new state is previous outs + outs occurring on play
                         runners))
  
  # Clean up 3 out state
  df$STATE1 = gsub(pattern = "^3[0-9]{3}", replacement = 3, df$STATE1)
  
  ## TRANSITION PROBABILITIES ##

  # Collect states
  states = df %>%
    select(STATE0, STATE1)

  # Find frequencies for each STATE0 
  freqs = states %>% group_by(STATE0) %>% summarise(n = n())

  # Find frequencies for each (STATE0, STATE1) combination
  combos = states %>% 
    group_by(STATE0, STATE1) %>% 
    mutate(n = n()) %>%
    distinct() 

  combos_wide = combos %>%
    pivot_wider(names_from = "STATE1", values_from = "n")
  combos_wide[is.na(combos_wide)] = 0

  # Compute transition probabilities iteratively
  P = matrix(data = NA, nrow = 24, ncol = 25)
  for (i in 1:24) {  # Indexes the rows
    starting_state = combos_wide$STATE0[i]
    n = freqs %>% filter(STATE0 == starting_state) %>% pull(n)
    for (j in 1:25) {  # Indexes the columns
      P[i, j] = pull(combos_wide[i, j + 1]) / n
    }
  }
  row.names(P) = combos_wide$STATE0
  colnames(P) = colnames(combos_wide)[2:26]
  ordered_states = freqs %>% arrange(desc(n)) %>% pull(STATE0)
  P = P[ordered_states, c(ordered_states, "3")]
  
  # Add 3 out row to P
  P = rbind(P, c(rep(0, 24), 1))
  row.names(P) = c(row.names(P)[1:24], "3")
  return(P)
}

P = make_chain(dat1)

# Ensure rows sum to 1
apply(P, 1, sum)
```

# Runs Scored Computations

```{r}
# Runs scored function
run_transitions = function(state0, state1) {
  o0 = as.numeric(substr(state0, 1, 1)); o1 = as.numeric(substr(state1, 1, 1))
  nrunners0 = as.numeric(substr(state0, 2, 2)) + as.numeric(substr(state0, 3, 3)) + as.numeric(substr(state0, 4, 4))
  nrunners1 = as.numeric(substr(state1, 2, 2)) + as.numeric(substr(state1, 3, 3)) + as.numeric(substr(state1, 4, 4))
  runs = (nrunners0 + o0 + 1) - (nrunners1 + o1)
  return(runs)
}

# Tests
run_transitions("0000", "0000")
run_transitions("0001", "1100")
run_transitions("1110", "2010")
run_transitions("0000", "1000")
run_transitions("1000", "2000")
run_transitions("2111", "2111")

# Runs scored matrix
ordered_states = row.names(P)[1:24]
runs = t(sapply(ordered_states, run_transitions, state1 = ordered_states))
runs = cbind(runs, rep(0, 24))  # For 3 out state
colnames(runs) = c(ordered_states, "3")
head(runs)
write.table(runs, "R.txt")
```

# Simulation

```{r}
# Simulate half inning
# Takes transition probability matrix P, run matrix R, and starting state start of form "XXXX"
sim_half_inning = function(P, R, start) {
  ordered_states = colnames(P)
  state0 = start
  idx_start = which(ordered_states == start)
  pi0 = rep(0, 25); pi0[idx_start] = 1   # Start in state start with probability 1
  # Initial out and run values
  outs = 0
  runs = 0
  states = c(state0)  # Store initial state
  
  # Simulate until 3 outs
  while (state0 != "3") {
    state_n_idx = sample(1:25, size = 1, prob = P[state0, ])  # Get index of new state
    state_n = ordered_states[state_n_idx]  # Get new state
    states = c(states, state_n)  # Store new state
    runs = runs + R[state0, state_n]  # Update runs
    state0 = state_n  # Update state0
  }
  # Store runs and states in a list and return
  res = list(runs = runs, states = states)
  if (runs == -1) {print(states)}
  return(res)
}

onesim = sim_half_inning(as.matrix(P), runs, "0000")
onesim
```

# Run Distributions

```{r, message=F}
n_inn = 10000 

# Gets run distribution
# Takes probability transition matrix P, run matrix R, starting state start, and n_inn, the number of innings to simulate
get_run_dist = function(P, R, start, n_inn) {
  run_dist = replicate(n_inn, sim_half_inning(P, R, start)$runs)
  return(run_dist)
}

# Store run distributions for each possible starting state
lst = list()
for (state in ordered_states) {
  run_dist_state = get_run_dist(P, runs, state, n_inn)
  lst[[state]] = run_dist_state
}

# Store plots
hists = lapply(ordered_states, 
               function(state) ggplot2::qplot(lst[[state]], 
                                              geom = "histogram", 
                                              xlab = state,
                                              binwidth = 0.25))

# Plot
plt = cowplot::plot_grid(plotlist = hists, nrow = 6, ncol = 4)
plt
```

```{r}
ggsave("run_dists.png")
```

# Run Expectancy

```{r}
# Get mean runs scored for each state
run_expectancies = sapply(lst, mean)

# Build run expectancy matrix
# base configuration
base_config = c("000", "100", "010", "001", "110", "101", "011", "111")
# Order run expectancies according to base configuration
run_expectancies = run_expectancies[c("0000", "1000", "2000", 
                                      "0100", "1100", "2100",
                                      "0010", "1010", "2010",
                                      "0001", "1001", "2001",
                                      "0110", "1110", "2110",
                                      "0101", "1101", "2101",
                                      "0011", "1011", "2011",
                                      "0111", "1111", "2111")]
RE = matrix(data = run_expectancies, nrow = 8, ncol = 3,
            dimnames = list(base_config,
                            c("0", "1", "2")),  # Outs
            byrow = T) %>%
  round(2)
RE
write.table(RE, "RE.txt")
```

### Aside: Fundamental Matrix

Upon reading a bit about absorbing Markov chains on [their Wikipedia page](https://en.wikipedia.org/wiki/Absorbing_Markov_chain), I discovered that one of their important properties is the expected number of visits to a transient state $j$ from a transient state $i$ before being absorbed. The $(i,j)$ entry of the *fundamental matrix* $N$ tells us exactly this, and is defined $N =(I_{24}-Q)^{-1}$, where $Q\in\mathbb{R}^{24 \times 24}$ is the probability transition matrix between the 24 transient states. Again, entry $N_{ij}$ is the expected number of times the chain is in state $j$ given the chain started in state $i$. Let's compute $N$ and show the first row, corresponding to starting in the `0000` state.

```{r}
Q = P[1:24, 1:24]  # Compute Q
N = solve((diag(24) - Q))  # Compute N
state0_visits = sort(N["0000",], decreasing = TRUE)  # First row of N
state0_visits
```

The expected number of visits to state `0000` before being absorbed is 1.05, the highest we see. This intuitively makes sense since we start in this state and occasionally visit it again when a solo home run is hit. The next two most frequently visited states are the one and two out states with bases empty at 0.76 and 0.61 times respectively, also intuitive. Meanwhile, the average number of visits to `0001` is merely 0.01, which again makes sense since triples are extremely rare events.

It's important to keep in mind that our Markov chain is an average over all teams. In other words, the transition probability matrix $P$, the run expectancy matrix $RE$, and the fundamental matrix $N$ all represent results for the average professional baseball team. In the last bit of this project, we will get a bit more granular and see if there is a difference in Markov chains for home and away teams. This requires comparing two Markov chains, one for home teams and one for away teams. Let's get to creating them now.

# Home and Away Markov chains

```{r}
# Get chain for home teams
home_dat = dat1 %>% 
  filter(BAT_HOME_ID == 1)
P_home = make_chain(home_dat)
head(P_home)

# Get chain for away teams
away_dat = dat1 %>%
  filter(BAT_HOME_ID == 0)
P_away = make_chain(away_dat)
head(P_away)
```

```{r}
apply(P_home, 1, sum)
apply(P_away, 1, sum)
```

Great, now we have a Markov chain for the home and away teams. Are they different? One naive check is to simply take their Frobenius norm, which is defined for any matrix $A$ as as $||A||_F=\sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}|a_{ij}|^2}$. We compute $||P_{\text{home}} - P_{\text{away}}||_F$:

```{r}
diff = norm(P_home - P_away, type = "F")  # Compute Frobenius norm
diff
```

Therefore the square root of the sum of squared differences between $P_{\text{home}}$ and $P_{\text{away}}$ is 2.24. But this number is just a number; it doesn't really tell us much. What would be more helpful is to simulate a runs scored distribution using $P_{\text{home}}$ and one using $P_{\text{away}}$ and compare how similar they are. Thankfully we've already built most of this functionality, which we put to use now.

```{r}
# Starting values
n_inn = 10000
start = "0000"

run_dist_home = get_run_dist(P_home, runs, start, n_inn)
run_dist_away = get_run_dist(P_away, runs, start, n_inn)
dists = data.frame(Team = c(rep("Home", n_inn), rep("Away", n_inn)), runs = c(run_dist_home, run_dist_away))

# Plot overlaid histograms
ha_dist = ggplot(dists, aes(x = runs, group = Team, fill = Team)) +
  geom_histogram(position = "identity", alpha=0.35, binwidth=0.25) + 
  theme_bw() +
  geom_vline(xintercept = mean(run_dist_home), color = "dodgerblue1") +
  geom_text(aes(label = paste("Home average: ", mean(run_dist_home)), x = 0.55, y = 3000), angle = 90, vjust = 1, color = "dodgerblue1") +
  geom_vline(xintercept = mean(run_dist_away), color = "palevioletred2") + 
  geom_text(aes(label = paste("Away average: ", mean(run_dist_away)), x = 0.55, y = 3000), angle = 90, vjust = -1, color = "palevioletred2") +
  labs(title = "Starting State = 0000; N = 10,000 Half Innings", x = "Runs", y = "Count")
ha_dist
```

```{r}
ggsave('HA_run_dist.png')
```

```{r}
# What are the standard deviations?
sd(run_dist_home)
sd(run_dist_away)

# The pooled SD of the difference of means is (by independence & using pooled formula)...
SD = sqrt((var(run_dist_home) + var(run_dist_away)) / 2)
SD
# 9 inning SD difference
9 * SD
```

```{r}
(var(run_dist_away) + var(run_dist_home)) / 2
```


### Aside: Modeling Runs Scored as Independent Poisson Random Variables

The difference in run scoring induced by the home and away Markov chains seems miniscule. However, an interesting idea now comes to mind: how can we model the total number of runs scored in a baseball game? Well, we can let $X = \text{number of runs scored in a game by the home team} \sim Pois(\lambda_1)$, and $Y = \text{number of runs scored in a game by the away team} \sim Pois(\lambda_2)$. Then we can estimate $\hat{\lambda}_1=9*0.5246=4.72$ and $\hat{\lambda}_2 =9* 0.4652=4.72$, since each team comes to bat 9 times, and the home team averages 0.5246 runs per inning while the away team averages 0.4652 So $X \sim Pois(4.72), Y \sim Pois(4.19)$, and the total number of runs scored in a game $R$ is $R = X + Y$, which looks like (but isn't quite) a mixture distribution. Nonetheless, we can still sample from $X$ and $Y$ individually and sum them to get $R$. This is super easy to simulate, so let's do so and visualize the results:

```{r}
n = 1000
# Draw  X
X = rpois(n, 4.72)
# Draw Y
Y = rpois(n, 4.19)
# Sum to get R
R = X + Y

# Distribution of R
ggplot() +
  geom_histogram(aes(R), binwidth = 0.5) +
  theme_bw() +
  geom_vline(xintercept = mean(R), color = "palevioletred2") + 
  geom_text(aes(label = paste("Average: ", mean(R)), x = 10.7, y = 143), color = "palevioletred2") +
  labs(x = "Runs", y = "Count", title = "Full Game Run Distribution")
```


How accurate is this model? Well, we can use our data to compute the average number of runs scored per game in 2021:

```{r}
avg_runs = dat1 %>%
  mutate(GAME_ID2 = paste0(GAME_ID, AWAY_TEAM_ID)) %>%
  group_by(GAME_ID2) %>%
  summarize(score = last(HOME_SCORE_CT) + last(AWAY_SCORE_CT)) %>%
  pull(score) %>%
  mean()
avg_runs
```

8.7 runs were scored per game in 2021, meaning this model is off by only 0.3 runs. Very nice!

In fact, $R=X+Y$ is the sum of independent Poissons if we assume $X$ and $Y$ are independent, which I believe is reasonable. Then instead of drawing separately from $X$ and $Y$ and summing them to get $R$, we have that $R \sim Pois(4.72 + 4.19) = Pois(8.91)$ on its own, which is simpler and nicer to work with. 